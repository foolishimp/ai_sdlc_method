# Competitive Analysis: AI SDLC Methodology

This document provides an evaluation matrix to justify the AI SDLC Methodology against other specification-driven or AI-assisted development approaches. The evaluation criteria are derived directly from the core problems the AI SDLC is designed to solve, as outlined in the project's `INTENT.md` document.

| Criterion | AI SDLC Methodology (This Project) | Traditional BDD/TDD | General AI Assistants (Copilot) | Agent-based AI Coders (Aider) | Model-Driven Engineering (MDE) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **1. Formal Methodology** | **Yes.** A formal, ontology-grounded methodology (Asset Graph, Universal Iterate) is the core of the system. | **Partial.** A methodology exists (red-green-refactor), but it's human-enforced and lacks a computational model of the project. | **No.** An assistant, not a methodology. It's used ad-hoc within the developer's existing workflow. | **Partial.** An implicit methodology exists (prompt -> code), but it's not formal, configurable, or persistent. | **Yes.** A highly formal methodology based on abstract models (e.g., UML) and transformations. |
| **2. Intent-to-Runtime Traceability** | **High.** Core design principle. Feature vectors create an explicit, auditable trajectory from `intent` to `requirements` to `code` to `test` to `runtime`. | **Medium.** Traceability exists from spec file (e.g., `.feature`) to test, but not formally connected to design documents, ADRs, or runtime telemetry. | **None.** No built-in traceability. The link between the thought process and the generated code is lost instantly. | **Low.** Traceability is limited to the user's prompt and the resulting git commit. No persistent link between higher-level intent and code changes over time. | **High.** Strong traceability from the model to the generated code is a primary feature. However, linking back from runtime or manual code changes is notoriously difficult. |
| **3. Context Persistence** | **High.** The Asset Graph and serialized context (ADRs, etc.) create a persistent, machine-readable project memory that is updated with every iteration. | **Low.** Context is stored in human-readable documents and the minds of the developers. No computational context persistence. | **Low.** Context is limited to the open files in the IDE and a small look-behind buffer. Forgets everything between sessions. | **Medium.** Maintains context within a single, active session/task. Does not build a long-term, evolving model of the entire project. | **High.** The model itself is the persistent context. However, it often becomes divorced from the reality of the running system. |
| **4. Enforced Quality Gates** | **High.** Quality is not optional. Evaluators (including tests, linters, and even other AIs) are required parameters for the `iterate` function, creating mandatory, configurable quality gates at every step. | **Partial.** A cultural practice, not a technical enforcement. A developer can still commit code without writing or passing tests. | **No.** The opposite is often true; it can hallucinate code that looks plausible but is incorrect or untested, increasing the need for human vigilance. | **No.** The user directs the agent. While it can run tests, the user can also instruct it to skip them or ignore failures. | **High.** The model's formal constraints and transformation rules act as a powerful, upfront quality gate. The primary risk is in the correctness of the model itself. |
| **5. Enterprise Audit & Compliance** | **High.** Designed for this. The immutable log of feature vector trajectories provides a deterministic audit trail, satisfying requirements like BCBS 239 or SOC 2. | **Low.** Relies on manual processes and combining data from multiple systems (Jira, Git, test runners) to build a compliance case. | **None.** Provides no mechanism for audit or compliance. Its "black box" nature can be a liability. | **Low.** Git history provides some audit, but there's no formal record of the decision-making process or guarantee that quality checks were performed. | **High.** The formal models and documented transformations are highly auditable, which is a key benefit for regulated industries. |
| **6. Full SDLC Coverage** | **High.** Designed to be a closed loop. The model explicitly includes connecting runtime telemetry and homeostasis back into the graph to generate new `intent`, covering the entire lifecycle. | **Low.** Primarily focused on the "development" phase of the lifecycle. Does not formally integrate with design, deployment, or operations. | **Low.** Only applies to the micro-loop of a developer writing a single piece of code. | **Low.** Focused on the development task itself. Does not extend to requirements gathering, deployment, or monitoring. | **Medium.** Covers design and development (generation) well. Often weak on deployment, operations, and feedback from the running system. |
| **7. Configurability** | **High.** The Asset Graph topology, evaluators, and constraints are all defined in YAML files, allowing the methodology to be tailored for different projects (e.g., `hotfix` vs. `spike` vs. `standard`). | **Medium.** The tools are configurable, but the core methodology is not. You can choose your test runner, but the red-green-refactor loop is constant. | **Low.** Some settings are available (e.g., inline suggestions), but the fundamental behavior cannot be changed. | **Low.** Some command-line options are available, but the core interaction model is fixed. | **Low.** MDE frameworks are notoriously rigid and heavyweight. Adapting them to a new project type is often a significant undertaking. |
| **8. AI Integration** | **Core.** AI is a fundamental primitive (`Agent` evaluator) and the engine of the `iterate` function. The entire system is designed to structure and control AI. | **None.** The methodology is human-centric. AI tools can be used *with* it, but are not part of it. | **Shallow.** AI is an "over-the-shoulder" assistant. It provides suggestions but is not a core part of the workflow or decision-making process. | **Core.** The AI agent is the primary actor responsible for writing and modifying code based on the user's specification. | **None.** A pre-AI methodology. AI could potentially be used to create the models, but it's not an integrated part. |

## Conclusion

The AI SDLC Methodology offers a unique synthesis of the formality of Model-Driven Engineering with the agility of BDD/TDD, while leveraging AI as a core, controlled engine rather than an ad-hoc assistant. Its primary competitive advantages are its **closed-loop view of the full SDLC**, its native support for **enterprise compliance and auditability**, and its **deep, structured integration of AI** as a first-class citizen in a formal, configurable, and persistent development framework.
