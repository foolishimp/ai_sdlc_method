# AI SDLC — Evaluator Defaults
# Version: 2.1.0
# Reference: AISDLC_IMPLEMENTATION_REQUIREMENTS.md REQ-EVAL-001, REQ-EVAL-002, REQ-EVAL-003
#
# Defines the three evaluator types and their default behaviour.
# Edge parameterisations can override these defaults.

---

evaluator_types:

  human:
    type: human
    description: "Human judgment — approval, rejection, or refinement guidance"
    mechanism: |
      Present the current asset candidate to the user.
      Ask for explicit approval, rejection, or refinement instructions.
      Record the human's decision and any feedback in iteration history.
    convergence: "User explicitly approves the asset"
    accountability: |
      AI assists, human decides. The human evaluator is the authority
      on all edges where human_required is true. The iterate agent
      NEVER auto-approves on behalf of the human.

  agent:
    type: agent
    description: "LLM-based assessment — coherence, completeness, gap analysis"
    mechanism: |
      Assess the current asset candidate against:
      - Source asset coherence (does it faithfully represent the source?)
      - Completeness (are all acceptance criteria addressed?)
      - Consistency (does it align with Context[]?)
      - Gap analysis (what's missing or inconsistent?)
    convergence: "No gaps detected, all criteria met, agent confirms quality"
    delta_report: |
      When delta > 0, provide specific, actionable feedback:
      - What is missing
      - What is inconsistent
      - What needs refinement
      - Suggested next action

  deterministic:
    type: deterministic
    description: "Automated checks — tests, schemas, linters, format validators"
    mechanism: |
      Run or invoke automated validation:
      - Compilation/parsing
      - Test execution (unit, integration, BDD)
      - Schema validation
      - Lint/style checks
      - REQ key tag presence and format
    convergence: "All checks pass"
    result_format: |
      Binary: pass/fail for each check.
      On failure, report:
      - Which check failed
      - The specific error
      - Remediation guidance

---

# Convergence composition rules
convergence_rules:
  # All evaluators on an edge must pass for convergence
  composition: "all_must_pass"

  # Order matters: run evaluators in the order specified in edge config
  # Typically: agent first (cheapest), then deterministic, then human (most expensive)
  ordering: "as_specified_in_edge_config"

  # If any evaluator fails, report the failure and suggest the next iteration
  on_failure: "report_delta_and_iterate"

  # Human evaluator is always last when present (reviews agent + deterministic results)
  human_always_last: true
